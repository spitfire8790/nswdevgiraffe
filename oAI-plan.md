
# Automation Pipeline for NSW Development Application Data

## Data Sources for NSW Development Applications

**NSW ePlanning Portal:** The primary data source is the NSW Planning Portal’s Development Application (DA) system. All councils in NSW have been mandated to use this central ePlanning Portal for lodging and tracking DAs since 1 July 2021 ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=The%20Online%20DA%20service%20API,Portal%20since%20January%20of%202019)) ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=Note%3A%20NSW%20Government%20has%20mandated,missing%20prior%20to%20that%20date)). The portal provides a unified register of DAs across the state, and an **Online DA Service API** offers a data feed of all applications lodged since January 2019 ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=The%20Online%20DA%20service%20API,Portal%20since%20January%20of%202019)). This means most new DAs (and many recent historical ones) can be accessed via one central platform. The portal’s open data listing confirms daily updates and comprehensive coverage (with only some pre-2021 cases possibly missing) ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=The%20Online%20DA%20service%20API,Portal%20since%20January%20of%202019)) ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=Applications,Geospatial%20Topic%20Structure%20Geospatial%20Coverage)). Using this official API or dataset is ideal for structured information (e.g. addresses, application types, dates) as a starting point.

**Individual Council DA Registers:** For DAs not covered in the state portal (e.g. older historical applications or edge cases), each local council traditionally has its own DA tracking website or planning register. These vary widely in format – some use standardized systems (e.g. **Pathway/TechnologyOne**, **Civica**, etc.), while others have custom portals. Many councils provide web search interfaces where one can look up applications by address or date. **Application Tracking Data Interchange Specification (ATDIS)** is a NSW-developed open format that some council systems support for exposing DA data ([
    Get Involved | Planning Alerts
](https://www.planningalerts.org.au/get_involved#:~:text=The%20most%20important%20thing%20you,freely%20available%20on%20the%20internet)). If a council’s system publishes an ATDIS feed (an XML/JSON API), it can be leveraged to retrieve application data in a consistent structure. In practice, however, not all councils implemented ATDIS, and **most councils historically did *not*** provide open, machine-readable DA data by default ([
    Coverage | Planning Alerts
](https://www.planningalerts.org.au/authorities#:~:text=To%20be%20able%20to%20display,into%20the%20Planning%20Alerts%20database)).

**Open Data Repositories:** A few councils have published DA datasets on open data portals. For example, the NSW Government’s Open Data portal (data.nsw.gov.au) mirrors the state ePlanning API dataset ([Online DA Data API | Data.NSW - Data.NSW](https://data.nsw.gov.au/data/dataset/online-da-data-api#:~:text=The%20Online%20DA%20service%20API,Portal%20since%20January%20of%202019)). Some councils (like City of Sydney) have occasionally released historical DA lists as open data or spreadsheets ([City of Sydney Development Applications - 2013 - 2014 — Data hub](https://data.opendatasoft.com/explore/dataset/development_applications@australiademo/?flg=en-us#:~:text=City%20of%20Sydney%20Development%20Applications,January%202013%20to%20December%202014)), but these are not uniformly available. Therefore, official feeds (like the ePlanning API) and the councils’ own tracking websites are the main sources for obtaining DA documents and details.

## Accessing and Scraping DA Registers and PDFs

**Using the NSW Planning API:** The **Online DA Data API** can be the backbone for initial data retrieval. It provides structured records of development applications statewide ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=The%20Online%20DA%20service%20API,Portal%20since%20January%20of%202019)). One can query this API (or download bulk data if available) to get fields like application IDs, addresses, descriptions, key dates (lodgement, determination), and possibly links or references to documents. This can significantly narrow the search: for each application record, the pipeline can then navigate to the portal’s public DA detail page or document repository to find associated PDFs. If the API provides direct document URLs or IDs, those can be used for download; otherwise, a secondary web scrape of the portal’s DA detail page may be required to extract the attachment links.

**Scraping Individual Council Sites:** For councils and DAs not fully in the state system (e.g. older DAs before 2019, or any council that maintains its own front-end), web scraping is a viable strategy. In fact, the OpenAustralia Foundation’s PlanningAlerts service uses scrapers for each council’s DA page because “the vast majority of councils don’t supply the data in a reusable, machine-readable format” ([
    Coverage | Planning Alerts
](https://www.planningalerts.org.au/authorities#:~:text=To%20be%20able%20to%20display,into%20the%20Planning%20Alerts%20database)). The pipeline should be prepared to mimic what these scrapers do: iterate through each council’s public DA listings and fetch documents. Typical strategies include:

- **Identify the DA listing or search page for each council:** Many councils have an “Application tracking” portal. Some allow querying by date range or application number; others might require crawling paginated lists. Determine how each council site lists DA numbers or provides links to application details.

- **Leverage ATDIS feeds when available:** If a council supports ATDIS 1.0, use its feed (usually a URL ending in something like `/atdis/1.0/`) to pull structured data of applications (including document links or at least application IDs). ATDIS was designed to standardize access to application info across NSW councils ([
    Get Involved | Planning Alerts
](https://www.planningalerts.org.au/get_involved#:~:text=The%20most%20important%20thing%20you,freely%20available%20on%20the%20internet)), so using it can simplify scraping for those councils that implement it.

- **Web scraping HTML pages:** In cases where no API/feed exists, use Python scraping tools to parse the HTML of the council’s DA tracking site:
  - Send HTTP requests (with `requests` or Scrapy) to the search results pages, possibly using date filters to get all records.
  - Parse the returned HTML (with **BeautifulSoup** or lxml) to extract application entries and their detail page links.
  - For each application detail page, find links to associated documents (often labeled “Attachments”, “Plans”, “Notice of Determination”, etc.). These links are typically PDFs.

- **Handling navigation and JavaScript:** Some council sites might require form submissions or have JavaScript loading of results (e.g. a map interface or dynamically loaded content). For these, the pipeline can either simulate the necessary requests (inspecting network calls to find an AJAX endpoint) or fall back to a tool like **Selenium** to render and scrape. In practice, many council DA pages are server-rendered or offer an alternative direct link (for accessibility) that can be scraped without a full browser. Using headless browsers is a last resort due to complexity and performance.

- **De-duplication and completeness:** Whether using the state API or scraping individual sites, the pipeline should track which applications have been processed (by an ID or URL) to avoid re-downloading the same PDFs repeatedly. It should also log any failures and possibly revisit them. For initial historical data load, a one-time scrape of each source (covering all available years) will be needed, and then the system can shift to incremental updates (e.g. checking daily or weekly for new entries, or using the API’s “since last update” capabilities if available).

**Downloading PDFs in Bulk:** Once the URLs of PDF files are known, the pipeline can use Python’s HTTP libraries to download them. Tools and tips for this stage:

- Utilize **`requests`** (or **`aiohttp`** for async batch downloads) to fetch PDF binaries. Ensure to handle large files and network hiccups (set timeouts, retry logic, etc.).
- Some document systems might require maintaining a session or passing along cookies from the scrape (especially if the document link isn’t a simple public URL). In such cases, re-use the session from the login or initial request that obtained the link.
- Respect rate limits and be mindful of server load – adding small delays or batching downloads will prevent overwhelming council servers. This is especially important when scraping dozens of councils in parallel.
- Store the PDFs with meaningful filenames or IDs (e.g. `<council>_<DA-number>_<document-name>.pdf`) for later reference. It might be wise to also save metadata like the source URL or application ID alongside each PDF, either in a database or as part of the file naming convention.

## PDF Parsing and Data Extraction Techniques

Once the PDFs are downloaded, the next challenge is **extracting structured information** from these documents. Development application PDFs can include a variety of content – scanned forms, typed reports, architectural drawings, tables, etc. The pipeline will need a combination of parsing methods:

- **Text Extraction from Native PDFs:** Many application documents (e.g. planning reports, consent notices, written proposals) are electronically generated PDFs containing selectable text. For these, a reliable PDF text extraction library like **PyMuPDF (fitz)** or **PDFPlumber** can pull out the text content. PyMuPDF is known for its speed and accuracy in extracting text (often outperforming PDFMiner-based tools in preserving layout) ([What's the Best Python Library for Extracting Text from PDFs? - Reddit](https://www.reddit.com/r/LangChain/comments/1e7cntq/whats_the_best_python_library_for_extracting_text/#:~:text=What%27s%20the%20Best%20Python%20Library,than%20PDFplumber%2C%20PyPDF2%2C%20and%20others)). Using such a library, the pipeline can obtain the full text of each PDF page. From the raw text, the next step is to parse out the specific fields needed:
  - **Addresses** and project descriptions are usually on the first page or header of an application/consent document. These can be found via keyword search (e.g. look for “Address:” or known patterns like a suburb or lot number).
  - **Zoning** information might be mentioned in planning reports (e.g. “the site is zoned R2 Low Density Residential under the Local Environmental Plan”). A simple approach is to look for the word “zone” or known zone codes (R1, B2, IN1, etc.) in the text.
  - **Floor space** figures (such as Gross Floor Area) and **dwelling counts/mix** might appear in a summary table or in text. Regular expressions can help identify numeric values followed by units (sqm, m²) or keywords like “GFA”, “floor space ratio”, “X units of housing (Y 1-bedroom, Z 2-bedroom…)”.
  - **Application type** (e.g. new development vs modification) is often in the title or description (for instance, modifications are labeled “Section 4.55” in NSW). These can be categorized by simple rules or keywords.
  - **Key Dates** (lodgement, exhibition, approval) might be listed in the document or can be taken from the metadata (the portal API itself likely provides lodgement date and determination date directly ([morph.io: planningalerts-scrapers/city_of_sydney](https://morph.io/planningalerts-scrapers/city_of_sydney#:~:text=,25))). If parsing from text, look for date patterns near words like “Lodged on” or “Determination Date”.
  - **Approval consent details:** The Notice of Determination (consent) PDF will contain the decision (Approved or Refused) and conditions of consent. Extracting the decision is straightforward (look for “Granted” or “Refused” text). The conditions are usually a numbered list of paragraphs. The pipeline could extract each condition as a separate item by splitting on the numbering scheme (e.g., “1.”, “2.” etc.). For deeper analytics, more advanced NLP could classify these conditions (e.g., which relate to landscaping, which to contributions, etc.), but at minimum the raw text of conditions can be captured.

- **Dealing with Scanned Documents (OCR):** Some PDFs, especially architectural plans or older documents, may essentially be scanned images with no embedded text. For these, an OCR step is required. Two popular OCR approaches in Python are:
  - **Tesseract OCR** via the `pytesseract` wrapper. Tesseract is an open-source engine supporting 100+ languages and can handle decent quality scans. It will convert images of text into machine-readable text. However, OCR can be error-prone with low-resolution scans or complex layouts ([Challenges You Will Face When Parsing PDFs With Python - How To Parse PDFs With Python - Seattle Data Guy](https://www.theseattledataguy.com/challenges-you-will-face-when-parsing-pdfs-with-python-how-to-parse-pdfs-with-python/#:~:text=Image)). It’s important to preprocess images (e.g., increase contrast, correct orientation) for better results.
  - **EasyOCR**, a deep-learning based OCR library, which has an easy API and supports 80+ languages ([Python OCR libraries for converting PDFs into editable text](https://ploomber.io/blog/pdf-ocr/#:~:text=,EasyOCR)). EasyOCR often has better accuracy on difficult fonts or noisy images since it uses neural networks (it was shown to even capture text from an embedded chart where Tesseract struggled ([Python OCR libraries for converting PDFs into editable text](https://ploomber.io/blog/pdf-ocr/#:~:text=match%20at%20L355%20The%20results,text%20from%20the%20pie%20chart))). The pipeline can use EasyOCR on plan drawings to read any text labels (like room names or dimensions).
  - **OCRmyPDF** is another useful tool if many PDFs are scans – it can deskew and run Tesseract on each page of a PDF, outputting a PDF with a searchable text layer ([Introduction — ocrmypdf 16.10.1.dev22+g6851ea7 documentation](https://ocrmypdf.readthedocs.io/en/latest/introduction.html#:~:text=documentation%20ocrmypdf,making%20scanned%20image%20PDFs%20searchable)). This is a convenient way to OCR lots of PDFs as a batch step, making them easier to parse subsequently with normal text extraction tools.

- **Table and Form Extraction:** Some structured data might appear in tabular form (for example, a schedule of dwellings by type, or a parking schedule). Libraries like **Camelot** or **Tabula** can detect and extract tables from PDF pages (if the PDF text is structured in a table grid). PDFPlumber also has table extraction capabilities. If these fail (for instance, if the table is an image scan), OCR combined with heuristic parsing might be needed. For critical tables (like a dwelling mix table listing number of 1-bed, 2-bed units), a custom parser can look for known column headers (“Dwelling Type”, “Count”) in the OCR’d text.

- **Natural Language Processing (NLP):** Once raw text is extracted from PDFs, NLP techniques can help refine and structure the information:
  - Use **spaCy** or **NLTK** to perform Named Entity Recognition on the text – this can identify entities like addresses, dates, and organizations. For example, spaCy’s models might tag an address or a lot/DP number, which helps confirm the site location.
  - Implement keyword-driven parsing for specific fields. For instance, to get the **floor space**, one might search the text for patterns like “Gross Floor Area: 1234”. To extract **approval conditions**, identify the section in the consent PDF where conditions start (often after a heading like “Conditions of Consent”) and then split by newlines or numbering.
  - If the format of documents varies a lot between councils, consider using an AI language model to locate and extract information. A prompt-based approach with a local large language model could be used (e.g., feeding the text and asking “What is the site address? How many dwellings are proposed?”). This can be a flexible way to handle variation without writing brittle rules. As an example, instead of manually coding rules for every layout, one could instruct an NLP model to find the “lodgement date” in the text; the model can infer it even if its position shifts ([Challenges You Will Face When Parsing PDFs With Python - How To Parse PDFs With Python - Seattle Data Guy](https://www.theseattledataguy.com/challenges-you-will-face-when-parsing-pdfs-with-python-how-to-parse-pdfs-with-python/#:~:text=and%20extraction%20rules)). For cost reasons, an **open-source LLM** (running locally) is preferable here – a fine-tuned model like **LLaMA 2** or **GPT4All** could potentially be used to extract fields from text without calling an external API.

In summary, the PDF parsing stage will likely combine **rule-based extraction** (for predictable fields and keywords) with **OCR** (for scanned content) and possibly **ML/NLP** for the less structured aspects. This multi-pronged approach ensures key data (addresses, zoning, floor areas, dates, etc.) are captured even from the diverse document types.

## AI Models for Floor Plans and Complex Content

Certain content types, notably **architectural floor plans** and lengthy **consent documents**, are complex and may benefit from specialized AI analysis:

- **Floor Plan Drawings (Visual Content):** Floor plan PDFs are essentially images (often vector drawings or scans) showing the layout of buildings. Extracting meaningful data (e.g. number of rooms, layout metrics) from these requires computer vision techniques. A straightforward step is to OCR any text on the plans – room labels (“Bedroom”, “Kitchen”), dimensions, or unit numbers can be read using OCR as mentioned. Beyond text, to truly interpret a floor plan (recognizing walls, doors, etc.), one can employ deep learning models from the field of graphical document analysis:
  - Research projects like **DeepFloorplan** (ICCV 2019) have demonstrated using convolutional neural networks to classify and detect structural components in floor plan images ([GitHub - zlzeng/DeepFloorplan](https://github.com/zlzeng/DeepFloorplan#:~:text=Introduction)). These models can identify rooms and boundaries, essentially converting an image into a set of rooms with types. For example, a multi-task network can learn to segment a floor plan into regions (rooms) and label each region as “bedroom”, “bathroom”, etc. ([GitHub - zlzeng/DeepFloorplan](https://github.com/zlzeng/DeepFloorplan#:~:text=Introduction)). Open-source code and pretrained models for such tasks exist ([zlzeng/DeepFloorplan - GitHub](https://github.com/zlzeng/DeepFloorplan#:~:text=zlzeng%2FDeepFloorplan%20,Guided)), which could be adapted and run locally (albeit with a GPU and significant processing time).
  - An alternative approach is to use general object detection frameworks (like **YOLO** or **Detectron2**) trained on floor plan symbols. With a custom training set, one could detect symbols for doors, windows, or count features like plumbing fixtures to infer number of bathrooms. This requires gathering annotated floor plan data, which might be an extensive project in itself.
  - If training a bespoke model is too heavy, a heuristic image processing approach can be tried: using **OpenCV** to detect lines (for walls) or connected components as separate rooms, etc. For instance, one might apply edge detection or line detection (Hough transform) to outline walls, then find enclosed polygons as rooms. Each room’s area could be approximated by pixel count (scaled by the drawing scale), and OCR on text within that polygon could reveal the room name. While feasible, this is complex and may not be as robust as a learned model, given the variety of plan drawing styles ([Floor Plan Recognition Using Computer Vision | by Maik Paixão | Medium](https://maikpaixao.medium.com/an%C3%A1lise-de-plantas-baixas-usando-intelig%C3%AAncia-artificial-40fa12e339be#:~:text=Although%20this%20type%20of%20problem,Computer%20Vision%20and%20Image%20Processing)).
  - **Output from floor plans:** Depending on project goals, the pipeline might extract metrics like total number of dwellings (by counting labels “Unit 1, 2, 3…”), number of floors (if multiple floor plan pages are present, or if labeled “Level 1”, “Level 2”), or floor space distribution (by reading dimensions). These tasks are advanced, but AI vision models can assist. For example, a deep learning service has been used to detect “doors, windows, and rooms from images or documents” automatically ([Floor Plan Recognition Using Computer Vision | by Maik Paixão | Medium](https://maikpaixao.medium.com/an%C3%A1lise-de-plantas-baixas-usando-intelig%C3%AAncia-artificial-40fa12e339be#:~:text=The%20Canadian%20company%20Measure%20Square,using%20the%20company%E2%80%99s%20own%20software)), showcasing that with the right model, key elements can be identified. Running such models locally would likely involve a Python deep learning stack (TensorFlow/PyTorch) and loading a pretrained model or training one.

- **Consent Forms and Reports (Language Content):** Long textual documents like Statements of Environmental Effects or Conditions of Consent can also be challenging – they may have inconsistent formatting or legal/technical language. AI can help here by:
  - **Document layout understanding models:** Transformers like **LayoutLMv3** (from Hugging Face) take both text and the layout into account ([Unlocking Document Intelligence: An Introduction to LayoutLMv3](https://indiaai.gov.in/article/unlocking-document-intelligence-an-introduction-to-layoutlmv3#:~:text=Unlocking%20Document%20Intelligence%3A%20An%20Introduction,to%20other%20pieces%20of%20text)). Fine-tuning such a model on a set of annotated consent documents could enable automatic extraction of fields (for example, tagging the “Decision: Approved/Refused” or segments of conditions). LayoutLM can be run locally with a GPU. Another model, **Donut (Document Understanding Transformer)**, is designed to parse documents end-to-end into JSON without needing OCR separate from the model.
  - **Language models for summarization/extraction:** A local instance of an LLM could summarize a multipart document or answer specific queries. For instance, one could prompt: “Summarize the main approval details of this DA” or “List all the conditions related to landscaping.” With a sufficiently strong model (and possibly dividing the document into chunks if it’s long), this could yield insights that are otherwise time-consuming to code for explicitly. Libraries like **Haystack** or **LangChain** allow setting up a local QA system over documents, using models like **Llama 2** or **Flan-T5** for processing text without external API calls.
  - **Rule-based NLP** is still useful in consent docs: e.g., identify if *deferred commencement* is a type of consent, or parse contribution amounts. Regular expressions and keyword searches can flag certain standard phrases in conditions (like “Prior to issue of Construction Certificate…” often starts a condition about CC stage).

**Balancing AI and Costs:** The emphasis is on locally run models to minimize cost. This means favoring open-source tools over commercial APIs. The good news is that many of the above options (Tesseract, EasyOCR, LayoutLM, Detectron2, etc.) are open-source and can be executed on local hardware. The pipeline might incorporate GPU acceleration for heavy tasks (like image analysis or large NLP models) but avoid paid APIs. The result is an AI-assisted parsing layer that can handle the variability in PDFs: simpler cases via straightforward parsing, and complex cases via learned pattern recognition.

## Automation Architecture and Integration

Designing the **automation pipeline architecture** requires considering data flow from extraction to end-user display, and ensuring it can run repeatedly for new data. Below is a recommended architecture:

- **Scraper & Ingestion Layer (Python):** A Python script or set of scripts will handle connecting to data sources and downloading PDFs. This could be orchestrated with a scheduler such as **Apache Airflow** or a simple cron job for regular runs. For example, an Airflow DAG could each day:
  1. Query the NSW Planning Portal API for any new or updated DAs since the last run (the API’s daily update frequency suits this ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=Applications,Geospatial%20Topic%20Structure%20Geospatial%20Coverage))).
  2. For each new DA (or each council of interest), ensure its documents are fetched. This might branch into council-specific tasks – e.g., one task to scrape City of Sydney’s site for new entries (if not fully on the API), another to handle a different council, etc.
  3. Download any new PDF files into a storage bucket or file system.
  4. Parse the PDFs and extract/update the structured data.

- **Data Storage:** Store structured data in a **database** for easy querying. A relational database (PostgreSQL or MySQL) could have tables like `applications` (one row per DA with address, dates, status, etc.), `documents` (one row per PDF with a foreign key to application and metadata like document type), and `extracted_data` (detailed attributes like floor space, dwelling count, etc., possibly in a JSON or in separate fields). Alternatively, a NoSQL store or Elasticsearch can be used if full-text search across documents is needed. The PDFs themselves can be stored on a file server or cloud storage (like AWS S3) with file paths or URLs recorded in the database.

- **Parsing and Analysis Layer:** Implement the PDF parsing as a pipeline that can be applied to each new document. This could be a separate Python service or part of the scraping script post-download. It would take a PDF file, determine the appropriate extraction method (text vs OCR vs image analysis), then output the structured info. For maintainability, this could leverage a **microservice pattern** – e.g., a containerized service that, given a PDF, returns JSON of extracted fields. This is where the AI models would be invoked as needed. By isolating parsing, you can re-run it on documents independently (useful if the extraction logic is improved over time – you can re-process older PDFs without re-downloading them).

- **Recurring Updates:** The pipeline should be idempotent and incremental. A common strategy is to keep track of the last seen application ID or last download timestamp. Each run, query only new records (the Planning API could potentially be filtered by lodgement date > last run, or simply get everything and skip those already in the DB). For council sites without a date filter, one might scrape the “last 1-2 weeks” of data and compare against stored records to catch any new ones. Since the system is meant to be ongoing, robust error handling is important – e.g., if one council site is down, that should log an error but not stop the whole pipeline, and it should retry on the next run.

- **Integration with TypeScript/React Frontend:** Expose the collected data to the frontend via a web service. A lightweight **REST API** built with Python (using a framework like **FastAPI** or Flask) can serve the data. This API would allow the React dashboard to query for things like “all DAs in a given suburb” or “details of DA XYZ including extracted metrics”. Depending on needs, GraphQL could also be used to give flexible queries to the frontend. The API will fetch data from the database and send it in JSON to the frontend. For example, an endpoint `/api/da/{id}` might return the address, description, status, and parsed details (like floorspace, dwelling mix) for a specific application, along with links to view the original PDFs if needed.

- **Dashboard Frontend:** The React/TypeScript dashboard can visualize the data – for instance, plotting DA locations on a map (using the address or coordinates if available), showing trends (using lodgement and approval dates for timelines), and overlaying with other spatial data (like zoning layers or historical approval rates). Since the question mentions *predictive analytics* and *future DA data overlay*, the frontend might also integrate GIS data (such as zoning maps from the NSW Planning Spatial Services) to allow overlays of proposals with land zoning or past decisions. The structured data extracted (e.g., floor space, dwelling count) can feed into predictive models – for example, predicting approval likelihood or processing time based on application features.

- **Scalability and Maintenance:** As the volume of data grows (imagine accumulating every DA’s documents across NSW), the architecture should remain maintainable:
  - Using a crawling framework like **Scrapy** for the scraping part can simplify handling hundreds of council sites by modularizing spiders for each site.
  - Dockerizing components (scraper, parser service, API service) ensures consistency across deployments.
  - For performance, consider that parsing PDFs (especially doing OCR or deep learning) is the most time-consuming step. This could be parallelized – e.g., using Python multiprocessing or distributing the work (one could push tasks to a **queue (Celery/RabbitMQ)** so that multiple worker processes handle PDF parsing concurrently). This way, adding more CPU/GPU resources can speed up the processing of large numbers of PDFs.
  - Caching results: since historical data will not change, once a PDF’s data is extracted, cache it (in the DB). The pipeline should not re-parse PDFs unless necessary (perhaps if a parsing improvement is implemented).

By following this architecture, the pipeline will systematically ingest raw DA documents and produce a clean, queryable dataset ready for analysis. The separation of concerns (scraping vs parsing vs serving to frontend) ensures that each part can be scaled or upgraded independently. For example, if a more accurate AI model for floor plans emerges, it can be integrated into the parsing layer without altering the scraping or the frontend.

## Comparison of Key Tools and Libraries

To implement the above, a variety of tools can be utilized. The table below summarizes some key tools and libraries in Python, along with their role and features, as relevant to this project:

| Tool/Library                | Purpose                      | Key Features / Notes |
|-----------------------------|------------------------------|----------------------|
| **Requests & BeautifulSoup**  | Web scraping (HTTP requests and HTML parsing) for council websites. | Simple and lightweight for static pages. Allows custom parsing logic for each site’s HTML structure. Might need to handle cookies and hidden form fields manually. |
| **Scrapy**                   | High-level web scraping framework. | Enables scalable crawling with concurrency, scheduling, and built-in parsing pipeline. Good for large-scale scrapes across many sites, with logging and auto-throttling. Steeper learning curve than BeautifulSoup for simple tasks. |
| **Selenium**                 | Browser automation for web scraping. | Can render JavaScript-heavy pages by controlling a headless browser (like Chrome/Firefox). Useful if a council site requires clicking through a map or running scripts to get data. Slower and more resource-intensive, so use only when necessary. |
| **NSW Planning Portal API** (REST/JSON) | Data feed of DAs (server-side API). | Official source for structured DA metadata ([Online DA Data API | Dataset | NSW Planning Portal](https://www.planningportal.nsw.gov.au/opendata/dataset/online-da-data-api#:~:text=The%20Online%20DA%20service%20API,Portal%20since%20January%20of%202019)). Offers daily updates and state-wide coverage. Reduces the need to scrape HTML for many councils. Requires connectivity and possibly an API key or registration to use (per NSW Planning’s terms). |
| **PyMuPDF (fitz)**           | PDF content extraction (text and images). | Fast, Python-friendly library to extract text, metadata, and even embed images from PDFs. Preserves layout; effective on digitally-created PDFs. Widely regarded as more effective than older PDFMiner-based tools for text extraction ([What's the Best Python Library for Extracting Text from PDFs? - Reddit](https://www.reddit.com/r/LangChain/comments/1e7cntq/whats_the_best_python_library_for_extracting_text/#:~:text=What%27s%20the%20Best%20Python%20Library,than%20PDFplumber%2C%20PyPDF2%2C%20and%20others)). |
| **PDFPlumber** (PDFMiner)    | PDF text and table extraction. | Pure Python PDF parsing with fine-grained control. Can extract tables by analyzing text positions. Slower on large files and may have trouble with unusual encodings, but useful for certain PDFs (especially if needing to inspect low-level layout data). |
| **Tesseract OCR** (`pytesseract`) | OCR for scanned documents. | Industry-standard open-source OCR engine. Good multi-language support (100+ languages). No cost to run locally. Requires image preprocessing for best results; accuracy ~90%+ on clear text, but can falter on messy scans ([Challenges You Will Face When Parsing PDFs With Python - How To Parse PDFs With Python - Seattle Data Guy](https://www.theseattledataguy.com/challenges-you-will-face-when-parsing-pdfs-with-python-how-to-parse-pdfs-with-python/#:~:text=Image)). |
| **EasyOCR**                 | Deep learning OCR for images/PDFs. | Open-source OCR library built on PyTorch. Supports 80+ languages with a neural network approach ([Python OCR libraries for converting PDFs into editable text](https://ploomber.io/blog/pdf-ocr/#:~:text=,EasyOCR)). Often more robust than Tesseract on complex imagery (e.g., it can read text from charts or stylized fonts) ([Python OCR libraries for converting PDFs into editable text](https://ploomber.io/blog/pdf-ocr/#:~:text=match%20at%20L355%20The%20results,text%20from%20the%20pie%20chart)). Slightly heavier (needs Torch and model weights), but runs locally on CPU/GPU. |
| **OCRmyPDF**                | OCR automation for PDFs.      | Wraps Tesseract to add an OCR text layer to PDFs, producing a PDF/A (searchable PDF). Simplifies processing of many scanned PDFs – after running, standard text extraction tools (like PyMuPDF) can be used on the output PDF text layer. |
| **spaCy**                   | Natural Language Processing. | Fast, robust NLP library with pre-trained models for English. Can identify entities (addresses, dates, person names) and do tokenization, which aids in parsing text blobs. Highly useful to pull structured data (like recognizing a date or address without regex). |
| **Hugging Face Transformers** (e.g. LayoutLM, BERT) | Advanced NLP and document AI models. | Offers state-of-the-art models that can be fine-tuned for tasks like document classification or information extraction. LayoutLM in particular is designed for PDFs, taking into account text position on the page. Requires ML expertise to fine-tune and a GPU to run efficiently. All models can be run offline. |
| **OpenCV** and **scikit-image** | Computer vision image processing. | Useful for analyzing floor plan images: e.g., detecting lines (for walls), contours (rooms), or filtering shapes. Can be combined with custom algorithms to measure dimensions or count objects. Doesn’t inherently know “what is a room” – needs custom logic or ML on top. |
| **DeepFloorplan / Custom CV Models** | Floor plan recognition via AI. | Open-source projects using deep learning to parse floor plans into structured data (rooms, doors, etc.) ([GitHub - zlzeng/DeepFloorplan](https://github.com/zlzeng/DeepFloorplan#:~:text=Introduction)). Can significantly automate understanding of architectural drawings. They need model training and GPU inference; using them involves setting up a ML pipeline (TensorFlow/PyTorch). |
| **Apache Airflow** (or cron jobs) | Workflow scheduling and automation. | Manages running the scraping/parsing tasks on a schedule. Airflow provides a UI and logging, making it easier to maintain a complex pipeline with dependencies (e.g., “scrape then parse then load”). A simpler cron + shell scripts could suffice for a smaller-scale or initial implementation. |
| **PostgreSQL / MySQL**       | Database for structured data storage. | Stores extracted data for quick lookup and filtering. Supports SQL queries which can power the dashboard (e.g., “show all approved DAs in 2023 in City X”). Use PostGIS (an extension) if doing geospatial queries (like map-based filtering by coordinates). |
| **FastAPI** (Python)         | Backend API framework.        | Efficient way to expose REST endpoints to the React frontend. Supports async operation (good if the API needs to fetch large data or call multiple services). Can serve as a middleware between the database and frontend, applying any final logic or aggregation. |
| **React (TypeScript)** Frontend | User interface for visualization. | Will display the data on a dashboard, possibly including interactive maps or charts. Leverages the API to get data. Not a data pipeline tool per se, but the ultimate consumer of the pipeline’s output. |

*Table:* **Key tools and libraries for each component of the DA pipeline, with their roles and features.** The choices above emphasize open-source solutions and locally run software to avoid recurring costs, aligning with the goal of minimizing external API usage.

## Conclusion

Implementing a comprehensive automation pipeline for NSW development application data is ambitious but feasible with the right strategy. By combining **official data sources** (the NSW Planning Portal API and any available council feeds) with **robust web scraping** for legacy systems, we can collect the universe of DA documents. From there, employing a mix of **PDF parsing libraries** and **AI models** allows extraction of valuable structured information – from simple fields like addresses and dates to complex insights like floor plan details and consent conditions. The use of **locally hosted AI/ML models** ensures the solution remains cost-effective at scale, while a well-designed **architecture** (with scheduled jobs, a parsing pipeline, and a database + API serving a React dashboard) ensures the data remains up-to-date for end users. This pipeline will enable predictive analytics and overlays (for example, correlating DAs with zoning or visualizing development trends) on a rich, continually growing dataset of NSW development applications, turning unstructured planning documents into actionable intelligence for planners, developers, and the community. 

